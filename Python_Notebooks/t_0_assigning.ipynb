{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done importing modules\n"
     ]
    }
   ],
   "source": [
    "##Importing packages we need##\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.feature import NaturalEarthFeature\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib import cm\n",
    "from matplotlib.cm import get_cmap\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "from numpy import *\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset, num2date\n",
    "import math\n",
    "import pygrib\n",
    "import cdsapi\n",
    "import imageio\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import metpy as mp\n",
    "import warnings\n",
    "import glob\n",
    "import dask\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Done importing modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Processing Function##\n",
    "\n",
    "def process_CF_data(ds_sfc):\n",
    "    '''This function does a lot of calculations and outputs the timeseries with CAA and FLF.'''\n",
    "    \n",
    "    ##Load in the datasets and read in variables and cords##\n",
    "\n",
    "    #This is for the surface level#\n",
    "    dew_2m = ds_sfc.d2m.metpy.sel(latitude=slice(latN,latS),longitude=slice(lonW,lonE))\n",
    "    uwnd_10m = ds_sfc.u10.metpy.sel(latitude=slice(latN,latS),longitude=slice(lonW,lonE))\n",
    "    vwnd_10m = ds_sfc.v10.metpy.sel(latitude=slice(latN,latS),longitude=slice(lonW,lonE))\n",
    "    tmpk_2m = ds_sfc.t2m.metpy.sel(latitude=slice(latN,latS),longitude=slice(lonW,lonE))\n",
    "    mslp = ds_sfc.msl.metpy.sel(latitude=slice(latN,latS),longitude=slice(lonW,lonE))\n",
    "    sfc_p = ds_sfc.sp.metpy.sel(latitude=slice(latN,latS),longitude=slice(lonW,lonE))\n",
    "\n",
    "    #Extract Coordinates#\n",
    "    lats = ds_sfc.latitude.metpy.sel(latitude=slice(latN,latS))\n",
    "    lons = ds_sfc.longitude.metpy.sel(longitude=slice(lonW,lonE))\n",
    "    lons_2D, lats_2D = meshgrid(lons,lats)\n",
    "    dx, dy = mpcalc.lat_lon_grid_deltas(lons, lats)\n",
    "\n",
    "    tmpk_2m\n",
    "    \n",
    "    ##Apply a spatial smoother to the variables so that synoptic-scale signals can be more readily observed (25)##\n",
    "\n",
    "    smoothing_var = 15\n",
    "\n",
    "    #Surface variables#\n",
    "    dew_2m = mpcalc.smooth_gaussian(dew_2m, smoothing_var)\n",
    "    uwnd_10m = mpcalc.smooth_gaussian(uwnd_10m, smoothing_var)\n",
    "    vwnd_10m = mpcalc.smooth_gaussian(vwnd_10m, smoothing_var)\n",
    "    tmpk_2m = mpcalc.smooth_gaussian(tmpk_2m, smoothing_var)\n",
    "    mslp = mpcalc.smooth_gaussian(mslp, smoothing_var)\n",
    "    sfc_p = mpcalc.smooth_gaussian(sfc_p, smoothing_var)\n",
    "    \n",
    "    ##Calculate additional variables to plot##\n",
    "\n",
    "    #Surface level variables#\n",
    "    thetaE = mpcalc.equivalent_potential_temperature(sfc_p, tmpk_2m, dew_2m) \n",
    "    thetaE_adv = mpcalc.advection(thetaE, uwnd_10m, vwnd_10m)\n",
    "\n",
    "    #Scale by a factor#\n",
    "    thetaE_adv_s = thetaE_adv * 1e4\n",
    "\n",
    "    #Select only CAA and WAA#\n",
    "    thetaE_adv_s_CAA = thetaE_adv_s.where(thetaE_adv_s < 0, other=np.nan)\n",
    "    thetaE_adv_s_WAA = thetaE_adv_s.where(thetaE_adv_s > 0, other=np.nan)\n",
    "\n",
    "    thetaE_adv_s\n",
    "    \n",
    "    ##Calculate Front Locator Function (FLF) using ThetaE##\n",
    "\n",
    "    def calculate_flf(thetaE, dx, dy):\n",
    "        '''Make an FLF function to calculate it for all gridpoints at each timestep.'''\n",
    "\n",
    "        #Step 1: Calculate the gradient of thetaE#\n",
    "        d0_dy, d0_dx = mpcalc.gradient(thetaE, deltas=(dy, dx)) \n",
    "\n",
    "        #Step 2: Calculate the magnitude of the gradient#\n",
    "        mag_grad_thetaE = np.sqrt((d0_dx**2) + (d0_dy**2))  # |grad(thetaE)|\n",
    "\n",
    "        #Step 3a: Calculate the gradient of the magnitude#\n",
    "        grad_step2_y, grad_step2_x = mpcalc.gradient(mag_grad_thetaE, deltas=(dy, dx))\n",
    "\n",
    "        #Step 3b: Redo but for Qn component only#\n",
    "        qx = grad_step2_x.copy()\n",
    "        qy = grad_step2_y.copy()\n",
    "        denominator = ((d0_dx) * (d0_dx)) + ((d0_dy) * (d0_dy))\n",
    "\n",
    "        Qn_x = (((qx) * (d0_dx) * (d0_dx)) + ((qy) * (d0_dy) * (d0_dx))) / (denominator)  #i component\n",
    "        Qn_y = (((qx) * (d0_dx) * (d0_dy)) + ((qy) * (d0_dy) * (d0_dy))) / (denominator)  #j component\n",
    "        #Qn_low = Qn_x + Qn_y\n",
    "        #Qn = Qn_low * 1e10\n",
    "\n",
    "        #Step 4: Calculate the divergence of the gradient of the magnitude with Qn component#\n",
    "        div_step4_Qn = mpcalc.divergence(Qn_x, Qn_y, dx=dx, dy=dy)\n",
    "        div_step4_Qn_convert = div_step4_Qn * 1e14  #What we need\n",
    "\n",
    "        return div_step4_Qn_convert\n",
    "\n",
    "    #Apply the function across all timesteps now#\n",
    "    FLF_data = xr.apply_ufunc(\n",
    "        calculate_flf, \n",
    "        thetaE, \n",
    "        input_core_dims=[['latitude', 'longitude']],  #Core dimensions over which to apply the function\n",
    "        output_core_dims=[['latitude', 'longitude']], #Core dimensions on the output\n",
    "        vectorize=True,  #Automatically vectorize the computation if needed\n",
    "        dask='parallelized',  #Use Dask for parallel computation if thetaE is a Dask-backed array\n",
    "        kwargs={'dx': dx, 'dy': dy})  #Additional keyword arguments for the function\n",
    "    \n",
    "    ##See the CF dataset and some days to select##\n",
    "    \n",
    "    ds_CF = xr.open_dataset('CF_xr.nc').metpy.parse_cf()\n",
    "    \n",
    "    ##The grid cell ranking method##\n",
    "\n",
    "    #Remake the spatial heatmap#\n",
    "    cold_front_counts = ds_CF['CF_Dataset'].sum(dim='time')\n",
    "\n",
    "    #Sort the counts to obtain ranks for each grid cell#\n",
    "    flattened_counts = cold_front_counts.values.flatten()\n",
    "    sorted_indices = np.argsort(-flattened_counts)  #Negative for descending sort\n",
    "\n",
    "    ranks = np.empty_like(sorted_indices)\n",
    "    ranks[sorted_indices] = np.arange(len(sorted_indices)) + 1  #Rank starts from 1\n",
    "\n",
    "    #Plot the heatmap plot underneath#\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    heatmap = ax.imshow(cold_front_counts, cmap='viridis', origin='lower') \n",
    "\n",
    "    plt.colorbar(heatmap, label='Number of Cold Fronts')\n",
    "    ax.set_xlabel('Longitude', fontsize=12)\n",
    "    ax.set_ylabel('Latitude', fontsize=12)\n",
    "    ax.set_title('Cold Front Occurrences Heatmap with Ranked Grid Cells', fontsize=16)\n",
    "\n",
    "    #Fix the lons and lats#\n",
    "    lat_vals = np.flip(ds_CF['latitude'].values)  \n",
    "    lon_vals = ds_CF['longitude'].values  \n",
    "\n",
    "    ax.set_xticks(np.arange(len(lon_vals)))\n",
    "    ax.set_yticks(np.arange(len(lat_vals)))\n",
    "    ax.set_xticklabels([f'{lon:.2f}' for lon in lon_vals])\n",
    "    ax.set_yticklabels([f'{lat:.2f}' for lat in lat_vals])\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    #Convert ranks to 2D grid and print them out on cells#\n",
    "    ranks_2d = ranks.reshape(cold_front_counts.shape)\n",
    "    lon = cold_front_counts['longitude'].values\n",
    "    lat = cold_front_counts['latitude'].values\n",
    "\n",
    "    #Overlay original smallest rectangle#\n",
    "    rect_domain2 = patches.Rectangle(\n",
    "\n",
    "        (5-0.5, 5-0.5), #(lon_min, lat_min)\n",
    "        13 - 5,  #width (lon_max - lon_min)\n",
    "        11 - 5,  #height (lat_max - lat_min)\n",
    "        linewidth=4,\n",
    "        edgecolor='black',\n",
    "        facecolor='none'\n",
    "    )\n",
    "\n",
    "    ax.add_patch(rect_domain2)\n",
    "\n",
    "    #Print out the ranks in each grid cell#\n",
    "    for i, row in enumerate(ranks_2d):   \n",
    "            for j, rank in enumerate(row):\n",
    "\n",
    "                if rank <= 238: \n",
    "                    ax.text(j, i, str(rank), ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "    #Now make a mask for the highest frequency 52 cells#\n",
    "    top_cells = np.dstack(np.unravel_index(np.argsort(ranks_2d.ravel()), ranks_2d.shape))[0][:52]\n",
    "\n",
    "    #Initialize the binary mask array#\n",
    "    binary_mask_tc = np.zeros_like(ranks_2d, dtype=bool)\n",
    "\n",
    "    #Set the top 52 cells in the mask to True#\n",
    "    for cell in top_cells:\n",
    "        binary_mask_tc[cell[0], cell[1]] = True\n",
    "\n",
    "    #Add outlines for the top 52 cells#\n",
    "    for cell in top_cells:\n",
    "        rect = patches.Rectangle((cell[1]-0.5, cell[0]-0.5), 1, 1, linewidth=2, edgecolor='purple', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    ##We want to extract the FLF values for the Eastern CO domain##\n",
    "\n",
    "    #First remove all the frontolysis values#\n",
    "    FLF_data_FG = FLF_data.where(FLF_data <= 0, other=np.nan)\n",
    "\n",
    "    #Obtain the CAA for Theta#\n",
    "    thetaE_CAA = thetaE_adv_s_CAA.copy()\n",
    "\n",
    "    #Define extents and subset#\n",
    "    ECO_area = [41, -105.25, 37, -102]  #Eastern CO  105.25\n",
    "\n",
    "    FLF_ECO = FLF_data_FG.sel(latitude=slice(ECO_area[0],ECO_area[2]),longitude=slice(ECO_area[1],ECO_area[3]))\n",
    "    thetaE_CAA_ECO = thetaE_CAA.sel(latitude=slice(ECO_area[0],ECO_area[2]),longitude=slice(ECO_area[1],ECO_area[3]))\n",
    "\n",
    "    #Define a start date#\n",
    "    start_date = str(FLF_ECO['time'][0].values)\n",
    "\n",
    "    #Create a second permanent mask using the 52 highest freq. gridcells and subset data#\n",
    "    binary_mask_tcf = np.flipud(binary_mask_tc)\n",
    "\n",
    "    FLF_top52flag = FLF_ECO * binary_mask_tcf\n",
    "    thetaE_CAA_top52flag = thetaE_CAA_ECO * binary_mask_tcf\n",
    "\n",
    "    #Accumulate the FLF, ThetaEadv for the different regions#\n",
    "    FLF_accumulated_top52 = abs(FLF_top52flag).sum(dim=['latitude', 'longitude'])\n",
    "    thetaE_CAA_accumulated_top52 = abs(thetaE_CAA_top52flag).sum(dim=['latitude', 'longitude'])\n",
    "\n",
    "    #Plotting with pandas dates#\n",
    "    dates = pd.date_range(start=start_date, periods=FLF_accumulated_top52.sizes['time'], freq='H')\n",
    "    dates = pd.to_datetime(dates)\n",
    "\n",
    "    FLF_timeseries = pd.Series(data=FLF_accumulated_top52.values, index=dates)\n",
    "    FLF_timeseries2 = pd.Series(data=thetaE_CAA_accumulated_top52.values, index=dates)\n",
    "\n",
    "     #Try an overlap with FLF and CAA now#\n",
    "\n",
    "    #Create a new DataArray that only contains FLF values where CAA is non-zero#\n",
    "    FLF_and_CAA = xr.where(thetaE_CAA_top52flag.values < 0, FLF_top52flag, 0)  \n",
    "    FLF_CAA_accumulated = abs(FLF_and_CAA).sum(dim=['latitude', 'longitude'])\n",
    "    FLF_CAA_timeseries = pd.Series(data=FLF_CAA_accumulated.values, index=dates)\n",
    "\n",
    "    FLF_timeseries_conv = FLF_CAA_timeseries * FLF_timeseries2   #combine orange and blue lines\n",
    "\n",
    "     #Plotting#\n",
    "\n",
    "    #Make plot 1 for T52#\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    ax2 = ax.twinx() \n",
    "    ax.plot(FLF_timeseries.index, FLF_timeseries.values, linewidth=3, color='black', label='FLF T52')\n",
    "    ax2.plot(FLF_timeseries2.index, FLF_timeseries2.values, linewidth=3, color='blue', label='CAA T52')\n",
    "    ax.plot(FLF_CAA_timeseries.index, FLF_CAA_timeseries.values, linewidth=3, color='orange', label='T52: FLF and CAA')\n",
    "    ax2.plot(FLF_timeseries_conv.index, FLF_timeseries_conv.values, linewidth=3, color='purple', linestyle='--', label='Weighted')\n",
    "\n",
    "    ax.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.set_xlabel('Time', fontsize=14)\n",
    "    ax.set_ylabel('Accumulated Scaled FLF', fontsize=14)\n",
    "    ax2.set_ylabel('Accumulated Scaled CAA', fontsize=14)  #Label for the second y-axis\n",
    "    ax.set_title(f'Timeseries of Accumulated FLF,CAA T52 Method for {dates[47]}', fontsize=16)  #change\n",
    "    ax.set_xlim([dates[12],dates[60]])\n",
    "    ax.grid(True)  \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(f'FLF_Mult_{dates[47]}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    #Make plot 2 for T52#\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    ax2 = ax.twinx() \n",
    "    ax.plot(FLF_timeseries.index, FLF_timeseries.values, linewidth=3, color='black', label='FLF T52')\n",
    "    ax2.plot(FLF_timeseries2.index, FLF_timeseries2.values, linewidth=3, color='blue', label='CAA T52')\n",
    "    ax.plot(FLF_CAA_timeseries.index, FLF_CAA_timeseries.values, linewidth=3, color='orange', label='T52: FLF and CAA')\n",
    "    ax.plot(FLF_timeseries_conv.index, FLF_timeseries_conv.values, linewidth=3, color='purple', linestyle='--', label='Weighted')\n",
    "\n",
    "    ax.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.set_xlabel('Time', fontsize=14)\n",
    "    ax.set_ylabel('Accumulated Scaled FLF', fontsize=14)\n",
    "    ax2.set_ylabel('Accumulated Scaled CAA', fontsize=14)  #Label for the second y-axis\n",
    "    ax.set_title(f'Timeseries of Accumulated FLF,CAA T52 Method for {dates[47]}', fontsize=16)  #change\n",
    "    ax.set_xlim([dates[12],dates[60]])\n",
    "    ax.grid(True)  \n",
    "    plt.tight_layout() \n",
    "    plt.savefig(f'CAA_Mult_{dates[47]}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return FLF_timeseries_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Declare time, level, lat/lon boundaries and loop through a 2-day period for Frontal Analysis##\n",
    "\n",
    "#Area and Time#\n",
    "colorado_area = [44, -112, 34, -99]  #some buffer on all sides\n",
    "latN = 44\n",
    "latS = 34\n",
    "lonW = -112    #Must be in degrees E (The western hemisphere is captured between 180 and 360 degrees east)\n",
    "lonE = -99\n",
    "level = 850    #What level do we want to look at FLF on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "        date    max_value             max_time\n",
      "0 1959-01-04  3741.203221  1959-01-02 12:00:00\n",
      "1 1990-01-04   160.218851  1990-01-03 18:00:00\n",
      "2 1989-01-08   580.408035  1989-01-07 10:00:00\n",
      "3 2007-01-13  1658.155443  2007-01-11 21:00:00\n",
      "4 1967-01-15   370.152775  1967-01-14 17:00:00\n",
      "\n",
      "Corrected DataFrame:\n",
      "        Date             Max Time\n",
      "0 2059-01-04  1959-01-02 12:00:00\n",
      "1 2021-01-15   2021-01-14 8:00:00\n",
      "2 1975-01-22  1975-01-21 14:00:00\n",
      "3 1993-01-24  1993-01-23 11:00:00\n",
      "4 1989-02-02  1989-02-01 14:00:00\n",
      "\n",
      "Updated DataFrame:\n",
      "        date    max_value             max_time\n",
      "0 1959-01-04  3741.203221  1959-01-02 12:00:00\n",
      "1 1990-01-04   160.218851  1990-01-03 18:00:00\n",
      "2 1989-01-08   580.408035  1989-01-07 10:00:00\n",
      "3 2007-01-13  1658.155443  2007-01-11 21:00:00\n",
      "4 1967-01-15   370.152775  1967-01-14 17:00:00\n"
     ]
    }
   ],
   "source": [
    "##Load in the Org and Manual tables and perform a replacement##\n",
    "\n",
    "# #Load the 2 datasets@\n",
    "# df_CF_MANMF = pd.read_csv('CF_Cases _ManF_MF.csv')\n",
    "# df_CF_MANMF\n",
    "\n",
    "# ds_CF_mf = pd.read_csv('Group_Tables/csv/CF_mf.csv')\n",
    "# ds_CF_mf\n",
    "\n",
    "# Load the original and manually corrected tables\n",
    "df_original = pd.read_csv('Group_Tables/csv/CF_unclear.csv')\n",
    "df_corrected = pd.read_csv('CF_Cases_ManF_UNC.csv')\n",
    "\n",
    "# Ensure date columns are in the same format\n",
    "df_original['date'] = pd.to_datetime(df_original['date'])\n",
    "df_corrected['Date'] = pd.to_datetime(df_corrected['Date'])\n",
    "\n",
    "# Display the original and corrected DataFrames\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_original.head())\n",
    "\n",
    "print(\"\\nCorrected DataFrame:\")\n",
    "print(df_corrected.head())\n",
    "\n",
    "# Merge the original and corrected DataFrames\n",
    "df_updated = df_original.merge(df_corrected[['Date', 'Max Time']], left_on='date', right_on='Date', how='left')\n",
    "\n",
    "# Update the original DataFrame with the corrected max_time values\n",
    "df_updated['max_time'] = df_updated['Max Time'].combine_first(df_updated['max_time'])\n",
    "\n",
    "# Format the max_time column to ensure zero-padded hours\n",
    "df_updated['max_time'] = pd.to_datetime(df_updated['max_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Drop the extra 'Date' and 'Max Time' columns used for merging\n",
    "df_updated = df_updated.drop(columns=['Date', 'Max Time'])\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file or overwrite the original one\n",
    "df_updated.to_csv('Group_Tables/csv/CF_unclear_updated.csv', index=False)\n",
    "\n",
    "print(\"\\nUpdated DataFrame:\")\n",
    "print(df_updated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Import the CF date groups and read the data in with the correct year##\n",
    "\n",
    "# #Load the CSV data into a pandas df#\n",
    "# CF_groups = pd.read_csv('CF_Groups.csv')\n",
    "\n",
    "# #Retain dates between January 1, 1950 and December 31, 2022#\n",
    "# def refine_year(date):\n",
    "    \n",
    "#     if date.year < 1950:\n",
    "#         return date + pd.DateOffset(years=100)  \n",
    "    \n",
    "#     elif date.year > 2022:\n",
    "#         return date - pd.DateOffset(years=100)  \n",
    "    \n",
    "#     return date\n",
    "\n",
    "# #Extract the columns and convert to datetime objects#\n",
    "# CF_groups['Good'] = pd.to_datetime(CF_groups['Good'], format='%m/%d/%y', errors='coerce').apply(refine_year)\n",
    "# CF_groups['Multi Front'] = pd.to_datetime(CF_groups['Multi Front'], format='%m/%d/%y', errors='coerce').apply(refine_year)\n",
    "# CF_groups['Unclear '] = pd.to_datetime(CF_groups['Unclear '], format='%m/%d/%y', errors='coerce').apply(refine_year)\n",
    "\n",
    "# #Convert to xr#\n",
    "# CF_groups_xr = CF_groups.to_xarray()\n",
    "# CF_groups_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Perform some data munging##\n",
    "\n",
    "#Read in the dataset#\n",
    "# df_CF_MANMF = pd.read_csv('CF_Cases _ManF_MF.csv')\n",
    "# df_CF_MANMF\n",
    "\n",
    "# #Shift the index to start from 1 instead of 0#\n",
    "# df_CF_good.index = df_CF_good.index + 1\n",
    "# df_CF_good\n",
    "\n",
    "#Read in the dataset#\n",
    "#df_CF_mf = pd.read_csv('CF_multifront_org.csv')\n",
    "# df_CF_mf\n",
    "\n",
    "# #Shift the index to start from 1 instead of 0#\n",
    "# df_CF_mf.index = df_CF_mf.index + 1\n",
    "# df_CF_mf[25:35]\n",
    "\n",
    "# #Read in the dataset#\n",
    "# df_CF_unclear = pd.read_csv('CF_unclear_org.csv')\n",
    "# df_CF_unclear\n",
    "\n",
    "# #Shift the index to start from 1 instead of 0#\n",
    "# df_CF_unclear.index = df_CF_unclear.index + 1\n",
    "# df_CF_unclear[80:90]\n",
    "\n",
    "# #####\n",
    "\n",
    "# #Drop some selected rows#\n",
    "# indices_to_delete = [88]\n",
    "# df_CF_unclear_nodup = df_CF_unclear.drop(indices_to_delete)\n",
    "# df_CF_unclear_nodup\n",
    "\n",
    "# #Reset the index#\n",
    "# df_CF_unclear_nodupr = df_CF_unclear_nodup.reset_index(drop=True)\n",
    "# df_CF_unclear_nodupr.index = df_CF_unclear_nodupr.index + 1\n",
    "\n",
    "# #save#\n",
    "# df_CF_unclear_nodupr.to_csv('CF_unclear.csv', index=False)\n",
    "#df_CF_unclear_nodupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now plot the distribution of max values and a frequency of CF based on passage time##\n",
    "\n",
    "#Extract hour from 'max_time' and create a new column#\n",
    "df_CF_unclear_nodupr['hour'] = pd.to_datetime(df_CF_unclear_nodupr['max_time']).dt.hour\n",
    "\n",
    "#Count the frequency of each hour#\n",
    "hour_counts = df_CF_unclear_nodupr['hour'].value_counts().sort_index()\n",
    "\n",
    "#Plot the frequency of cold fronts by hour#\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(hour_counts.index, hour_counts.values, color='skyblue', edgecolor='black')\n",
    "plt.title('Frequency of \"Unclear\" Cold Fronts by Hour', fontsize=18)\n",
    "plt.xlabel('UTC Time', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xticks(range(24))\n",
    "plt.grid(axis='y')\n",
    "plt.text(0.98, 0.95, 'N=333', horizontalalignment='right', verticalalignment='top', transform=plt.gca().transAxes, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of max values\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.hist(df_CF_unclear_nodupr['max_value'], bins=30, color='orange', edgecolor='black')\n",
    "plt.title('Distribution of Max Values Unclear', fontsize=18)\n",
    "plt.xlabel('Max Value', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.grid(axis='y')\n",
    "plt.text(0.98, 0.95, 'N=333', horizontalalignment='right', verticalalignment='top', transform=plt.gca().transAxes, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Find the duplicates##\n",
    "\n",
    "# # #Round and Check for duplicates#\n",
    "# df_CF_good_r = df_CF_good.copy()\n",
    "# df_CF_good_r['max_value'] = df_CF_good_r['max_value'].round()\n",
    "\n",
    "# duplicates = df_CF_good_r[df_CF_good_r.duplicated('max_value', keep=False)]\n",
    "# duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_CF_unclear_nodupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now lets manually fix some dates and cases##\n",
    "\n",
    "# row = None\n",
    "\n",
    "# #Change the max time and value#\n",
    "# df_CF_good.at[row, 'max_time'] = '2022-01-01 00:00:00'\n",
    "# df_CF_good.at[row, 'max_value'] = 1234.56\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now put everything under a loop##\n",
    "\n",
    "#Use the \"good\" dates#\n",
    "results_unclear = []\n",
    "\n",
    "for date in CF_groups_xr['Unclear '].values:\n",
    "    \n",
    "    py_date = pd.to_datetime(date).to_pydatetime()\n",
    "    formatted_date = py_date.strftime('%m_%d_%Y')   #This will format the date as 'MM_DD_YYYY'\n",
    "    file_path = f\"CF_DATA/ERA5_{formatted_date}.nc\"\n",
    "    \n",
    "    try:\n",
    "        #Read in the data#\n",
    "        ds_sfc = xr.open_dataset(file_path).metpy.parse_cf()\n",
    "        \n",
    "        #Apply the function#\n",
    "        FLF_timeseries_conv = process_CF_data(ds_sfc)  \n",
    "        \n",
    "        #Locate the maximum value and time#\n",
    "        max_index = FLF_timeseries_conv[12:60].idxmax()\n",
    "        max_value = FLF_timeseries_conv[max_index]\n",
    "        \n",
    "        #Store the results#\n",
    "        results_unclear.append({'date': date, 'max_value': max_value, 'max_time': max_index})\n",
    "        print(f\"The maximum value is {max_value} and it occurs on {max_index}\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        \n",
    "        print(f\"File not found for date: {formatted_date}\")\n",
    "        \n",
    "        continue\n",
    "        \n",
    "#Convert results to DataFrame or xarray Dataset#\n",
    "results_df_unclear = pd.DataFrame(results_unclear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Convert to netcdf#\n",
    "\n",
    "# df_CF_mf_nodupr\n",
    "# df_CF_mf_nodupr.to_csv('CF_unclear.csv', index=False)\n",
    "\n",
    "# results_ds_unclear = results_df_unclear.set_index('date').to_xarray()\n",
    "# netcdf_path = 'CF_t0_unclear'\n",
    "# results_ds_unclear.to_netcdf(netcdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synoptic_f23",
   "language": "python",
   "name": "synoptic_f23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
